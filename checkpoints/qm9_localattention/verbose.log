Fold 0
{'activation': 'ReLU',
 'atom_messages': True,
 'batch_size': 50,
 'bias': False,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'cuda': True,
 'data_path': 'data/qm9.csv',
 'dataset_type': 'regression',
 'depth': 3,
 'dropout': 0.0,
 'ensemble_size': 1,
 'epochs': 30,
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': 0,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'rmse',
 'minimize_score': True,
 'multiclass_num_classes': 3,
 'no_cache': False,
 'num_folds': 1,
 'num_lrs': 1,
 'quiet': True,
 'save_dir': 'checkpoints/qm9_localattention/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'split_sizes': [0.8, 0.1, 0.1],
 'split_type': 'scaffold_balanced',
 'test': False,
 'test_fold_index': None,
 'undirected': False,
 'use_compound_names': False,
 'use_input_features': None,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 12
Splitting data with seed 0
Total scaffolds = 15,989 | train scaffolds = 13,406 | val scaffolds = 1,626 | test scaffolds = 957
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ 3.06770674e+00,  7.52033183e+01, -2.55926597e-01, -5.90761537e-03,
        2.50018624e-01,  1.52537269e+03,  1.51277474e-01,  3.52361516e+01,
       -4.16967530e+02, -4.16957378e+02, -4.16956434e+02, -4.17003691e+02]), array([13998, 13998, 13998, 13998, 13998, 13998, 13998, 13998, 13998,
       13998, 13998, 13998])), (array([ 2.33190000e+00,  7.53500000e+01, -2.14400000e-01, -1.70000000e-02,
        1.97400000e-01,  1.10743790e+03,  1.62372000e-01,  3.06960000e+01,
       -4.19191774e+02, -4.19183938e+02, -4.19182994e+02, -4.19224242e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 1.18850000e+00,  6.82200000e+01, -2.56400000e-01, -1.67000000e-02,
        2.39700000e-01,  1.11738480e+03,  1.24501000e-01,  2.86110000e+01,
       -4.74958657e+02, -4.74951049e+02, -4.74950105e+02, -4.74991316e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 5.00000000e-04,  8.44500000e+01, -2.18100000e-01,  8.30000000e-03,
        2.26400000e-01,  1.05648860e+03,  1.60839000e-01,  2.87300000e+01,
       -3.48675405e+02, -3.48668704e+02, -3.48667760e+02, -3.48706071e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 4.80930000e+00,  6.87400000e+01, -2.42900000e-01, -9.40000000e-03,
        2.33500000e-01,  9.29304000e+02,  1.26493000e-01,  2.60920000e+01,
       -4.37806287e+02, -4.37799843e+02, -4.37798899e+02, -4.37837025e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 4.92930000e+00,  7.09100000e+01, -1.99200000e-01, -2.50000000e-02,
        1.74300000e-01,  1.06849570e+03,  1.03158000e-01,  2.92830000e+01,
       -4.70081061e+02, -4.70073397e+02, -4.70072453e+02, -4.70113072e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 1.93840000e+00,  7.22900000e+01, -2.40600000e-01,  9.35000000e-02,
        3.34100000e-01,  8.22189000e+02,  1.52933000e-01,  2.47460000e+01,
       -4.01944215e+02, -4.01938591e+02, -4.01937647e+02, -4.01973852e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 5.06714286e-01,  7.50257143e+01, -2.07814286e-01,  8.42714286e-02,
        2.92114286e-01,  9.27218429e+02,  1.65630143e-01,  2.93787143e+01,
       -4.02603883e+02, -4.02596908e+02, -4.02595964e+02, -4.02634689e+02]), array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])), (array([ 1.99720000e+00,  8.18300000e+01, -2.15000000e-01,  8.90000000e-03,
        2.23900000e-01,  1.16949050e+03,  1.60685000e-01,  2.95760000e+01,
       -3.85827859e+02, -3.85820513e+02, -3.85819568e+02, -3.85859969e+02]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])), (array([ 2.97674286e+00,  7.03057143e+01, -2.59271429e-01,  2.23285714e-02,
        2.81628571e-01,  1.01687567e+03,  1.41121286e-01,  2.89358571e+01,
       -4.38610121e+02, -4.38602495e+02, -4.38601551e+02, -4.38642571e+02]), array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]))]
Total size = 133,885 | train size = 107,108 | val size = 13,388 | test size = 13,389
Fitting scaler
Building model 0
MoleculeModel(
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=133, out_features=300, bias=False)
      (W_h): Linear(in_features=314, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=12, bias=True)
  )
)
Number of parameters = 358,212
Moving model to cuda
Epoch 0
Loss = 2.0542e-02, PNorm = 34.1690, GNorm = 1.3542, lr_0 = 1.0210e-04
Loss = 1.7782e-02, PNorm = 34.1708, GNorm = 1.6582, lr_0 = 1.0420e-04
Loss = 1.6288e-02, PNorm = 34.1747, GNorm = 2.0328, lr_0 = 1.0630e-04
Loss = 1.4524e-02, PNorm = 34.1797, GNorm = 1.5135, lr_0 = 1.0840e-04
Loss = 1.4159e-02, PNorm = 34.1853, GNorm = 1.1319, lr_0 = 1.1050e-04
Loss = 1.3870e-02, PNorm = 34.1918, GNorm = 1.3684, lr_0 = 1.1261e-04
Loss = 1.2797e-02, PNorm = 34.1994, GNorm = 1.5373, lr_0 = 1.1471e-04
Loss = 1.1488e-02, PNorm = 34.2074, GNorm = 3.0356, lr_0 = 1.1681e-04
Loss = 1.1075e-02, PNorm = 34.2147, GNorm = 2.2132, lr_0 = 1.1891e-04
Loss = 1.1807e-02, PNorm = 34.2220, GNorm = 3.8373, lr_0 = 1.2101e-04
Loss = 1.1072e-02, PNorm = 34.2298, GNorm = 3.1323, lr_0 = 1.2311e-04
Loss = 9.7915e-03, PNorm = 34.2373, GNorm = 6.9811, lr_0 = 1.2521e-04
Loss = 9.0980e-03, PNorm = 34.2436, GNorm = 3.7497, lr_0 = 1.2731e-04
Loss = 1.0033e-02, PNorm = 34.2488, GNorm = 4.9467, lr_0 = 1.2941e-04
Loss = 9.0139e-03, PNorm = 34.2545, GNorm = 2.4994, lr_0 = 1.3151e-04
Loss = 8.8470e-03, PNorm = 34.2609, GNorm = 8.5232, lr_0 = 1.3361e-04
Loss = 9.2119e-03, PNorm = 34.2669, GNorm = 3.7835, lr_0 = 1.3571e-04
Loss = 8.6994e-03, PNorm = 34.2719, GNorm = 3.4066, lr_0 = 1.3782e-04
Loss = 8.0960e-03, PNorm = 34.2776, GNorm = 8.2630, lr_0 = 1.3992e-04
Loss = 7.0307e-03, PNorm = 34.2827, GNorm = 4.3405, lr_0 = 1.4202e-04
Loss = 8.8231e-03, PNorm = 34.2878, GNorm = 3.2566, lr_0 = 1.4412e-04
Loss = 7.2931e-03, PNorm = 34.2938, GNorm = 3.8274, lr_0 = 1.4622e-04
Loss = 9.4324e-03, PNorm = 34.2981, GNorm = 4.5411, lr_0 = 1.4832e-04
Loss = 9.3665e-03, PNorm = 34.3021, GNorm = 3.7054, lr_0 = 1.5042e-04
Loss = 7.6121e-03, PNorm = 34.3068, GNorm = 3.4043, lr_0 = 1.5252e-04
Loss = 7.2892e-03, PNorm = 34.3124, GNorm = 8.1366, lr_0 = 1.5462e-04
Loss = 9.2585e-03, PNorm = 34.3173, GNorm = 3.6082, lr_0 = 1.5672e-04
Loss = 6.9278e-03, PNorm = 34.3217, GNorm = 3.1160, lr_0 = 1.5882e-04
Loss = 6.1988e-03, PNorm = 34.3261, GNorm = 2.6270, lr_0 = 1.6092e-04
Loss = 6.8433e-03, PNorm = 34.3301, GNorm = 6.3750, lr_0 = 1.6303e-04
Loss = 7.5906e-03, PNorm = 34.3351, GNorm = 2.7859, lr_0 = 1.6513e-04
Loss = 7.5973e-03, PNorm = 34.3396, GNorm = 2.9652, lr_0 = 1.6723e-04
Loss = 6.4744e-03, PNorm = 34.3431, GNorm = 2.0663, lr_0 = 1.6933e-04
Loss = 6.7205e-03, PNorm = 34.3479, GNorm = 2.7324, lr_0 = 1.7143e-04
Loss = 8.4623e-03, PNorm = 34.3526, GNorm = 4.2286, lr_0 = 1.7353e-04
Loss = 6.6208e-03, PNorm = 34.3576, GNorm = 2.4583, lr_0 = 1.7563e-04
Loss = 7.2327e-03, PNorm = 34.3626, GNorm = 5.8702, lr_0 = 1.7773e-04
Loss = 6.6817e-03, PNorm = 34.3672, GNorm = 4.0507, lr_0 = 1.7983e-04
Loss = 6.3573e-03, PNorm = 34.3724, GNorm = 3.5243, lr_0 = 1.8193e-04
Loss = 6.5422e-03, PNorm = 34.3773, GNorm = 3.9000, lr_0 = 1.8403e-04
Loss = 8.5858e-03, PNorm = 34.3830, GNorm = 3.2730, lr_0 = 1.8613e-04
Loss = 7.8083e-03, PNorm = 34.3895, GNorm = 4.8289, lr_0 = 1.8824e-04
Loss = 7.5693e-03, PNorm = 34.3959, GNorm = 3.1817, lr_0 = 1.9034e-04
Loss = 6.4265e-03, PNorm = 34.4017, GNorm = 7.1970, lr_0 = 1.9244e-04
Loss = 7.7392e-03, PNorm = 34.4075, GNorm = 3.1856, lr_0 = 1.9454e-04
Loss = 5.8627e-03, PNorm = 34.4141, GNorm = 5.3387, lr_0 = 1.9664e-04
Loss = 5.7561e-03, PNorm = 34.4205, GNorm = 2.3015, lr_0 = 1.9874e-04
Loss = 7.0241e-03, PNorm = 34.4277, GNorm = 5.9531, lr_0 = 2.0084e-04
Loss = 7.4615e-03, PNorm = 34.4329, GNorm = 3.5596, lr_0 = 2.0294e-04
Loss = 6.7195e-03, PNorm = 34.4403, GNorm = 3.9351, lr_0 = 2.0504e-04
Loss = 6.7944e-03, PNorm = 34.4467, GNorm = 6.6780, lr_0 = 2.0714e-04
Loss = 6.5725e-03, PNorm = 34.4547, GNorm = 2.7460, lr_0 = 2.0924e-04
Loss = 6.5887e-03, PNorm = 34.4610, GNorm = 5.3530, lr_0 = 2.1134e-04
Loss = 7.0134e-03, PNorm = 34.4666, GNorm = 5.3232, lr_0 = 2.1345e-04
Loss = 6.4846e-03, PNorm = 34.4733, GNorm = 3.8775, lr_0 = 2.1555e-04
Loss = 6.5735e-03, PNorm = 34.4801, GNorm = 3.4075, lr_0 = 2.1765e-04
Loss = 7.0702e-03, PNorm = 34.4863, GNorm = 3.2542, lr_0 = 2.1975e-04
Loss = 5.7795e-03, PNorm = 34.4950, GNorm = 3.2359, lr_0 = 2.2185e-04
Loss = 5.9803e-03, PNorm = 34.5029, GNorm = 2.8181, lr_0 = 2.2395e-04
Loss = 6.9956e-03, PNorm = 34.5099, GNorm = 5.0861, lr_0 = 2.2605e-04
Loss = 6.4409e-03, PNorm = 34.5168, GNorm = 6.2899, lr_0 = 2.2815e-04
Loss = 7.2777e-03, PNorm = 34.5241, GNorm = 4.5734, lr_0 = 2.3025e-04
Loss = 7.1584e-03, PNorm = 34.5307, GNorm = 3.6055, lr_0 = 2.3235e-04
Loss = 7.9116e-03, PNorm = 34.5378, GNorm = 4.4226, lr_0 = 2.3445e-04
Loss = 5.0014e-03, PNorm = 34.5451, GNorm = 2.7860, lr_0 = 2.3655e-04
Loss = 6.7298e-03, PNorm = 34.5521, GNorm = 3.8902, lr_0 = 2.3866e-04
Loss = 7.9345e-03, PNorm = 34.5590, GNorm = 4.1115, lr_0 = 2.4076e-04
Loss = 6.1315e-03, PNorm = 34.5663, GNorm = 3.1989, lr_0 = 2.4286e-04
Loss = 6.6445e-03, PNorm = 34.5714, GNorm = 6.9054, lr_0 = 2.4496e-04
Loss = 5.8677e-03, PNorm = 34.5777, GNorm = 3.4970, lr_0 = 2.4706e-04
Loss = 6.7908e-03, PNorm = 34.5871, GNorm = 5.8704, lr_0 = 2.4916e-04
